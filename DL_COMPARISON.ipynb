{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlOSMHVuLWiu",
        "outputId": "c1228a29-5c8c-4abc-c34d-c9084a86a93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Updated Paper Model...\n",
            "Epoch 1/45, Loss: 1.8894, Training Accuracy: 37.08%\n",
            "Validation Accuracy after Epoch 1: 46.76%\n",
            "Epoch 2/45, Loss: 2.1687, Training Accuracy: 22.92%\n",
            "Validation Accuracy after Epoch 2: 15.60%\n",
            "Epoch 3/45, Loss: 1.9071, Training Accuracy: 30.77%\n",
            "Validation Accuracy after Epoch 3: 36.70%\n",
            "Epoch 4/45, Loss: 1.8357, Training Accuracy: 33.54%\n",
            "Validation Accuracy after Epoch 4: 32.23%\n",
            "Epoch 5/45, Loss: 1.7538, Training Accuracy: 35.89%\n",
            "Validation Accuracy after Epoch 5: 16.17%\n",
            "Epoch 6/45, Loss: 1.7358, Training Accuracy: 36.66%\n",
            "Validation Accuracy after Epoch 6: 37.77%\n",
            "Epoch 7/45, Loss: 1.7030, Training Accuracy: 37.84%\n",
            "Validation Accuracy after Epoch 7: 37.48%\n",
            "Epoch 8/45, Loss: 1.6764, Training Accuracy: 38.57%\n",
            "Validation Accuracy after Epoch 8: 41.35%\n",
            "Epoch 9/45, Loss: 1.6589, Training Accuracy: 39.17%\n",
            "Validation Accuracy after Epoch 9: 41.76%\n",
            "Epoch 10/45, Loss: 1.6503, Training Accuracy: 39.63%\n",
            "Validation Accuracy after Epoch 10: 39.41%\n",
            "Epoch 11/45, Loss: 1.6451, Training Accuracy: 39.67%\n",
            "Validation Accuracy after Epoch 11: 42.62%\n",
            "Epoch 12/45, Loss: 1.6321, Training Accuracy: 40.17%\n",
            "Validation Accuracy after Epoch 12: 42.73%\n",
            "Epoch 13/45, Loss: 1.6090, Training Accuracy: 40.91%\n",
            "Validation Accuracy after Epoch 13: 41.38%\n",
            "Epoch 14/45, Loss: 1.6015, Training Accuracy: 41.16%\n",
            "Validation Accuracy after Epoch 14: 36.89%\n",
            "Epoch 15/45, Loss: 1.5946, Training Accuracy: 41.60%\n",
            "Validation Accuracy after Epoch 15: 42.75%\n",
            "Epoch 16/45, Loss: 1.5819, Training Accuracy: 41.77%\n",
            "Validation Accuracy after Epoch 16: 40.10%\n",
            "Epoch 17/45, Loss: 1.5749, Training Accuracy: 42.00%\n",
            "Validation Accuracy after Epoch 17: 41.80%\n",
            "Epoch 18/45, Loss: 1.5801, Training Accuracy: 41.88%\n",
            "Validation Accuracy after Epoch 18: 39.80%\n",
            "Epoch 19/45, Loss: 1.5667, Training Accuracy: 42.37%\n",
            "Validation Accuracy after Epoch 19: 43.57%\n",
            "Epoch 20/45, Loss: 1.5641, Training Accuracy: 42.41%\n",
            "Validation Accuracy after Epoch 20: 43.44%\n",
            "Epoch 21/45, Loss: 1.4833, Training Accuracy: 45.65%\n",
            "Validation Accuracy after Epoch 21: 45.35%\n",
            "Epoch 22/45, Loss: 1.4795, Training Accuracy: 45.84%\n",
            "Validation Accuracy after Epoch 22: 45.48%\n",
            "Epoch 23/45, Loss: 1.4783, Training Accuracy: 45.78%\n",
            "Validation Accuracy after Epoch 23: 45.53%\n",
            "Epoch 24/45, Loss: 1.4777, Training Accuracy: 45.84%\n",
            "Validation Accuracy after Epoch 24: 45.43%\n",
            "Epoch 25/45, Loss: 1.4771, Training Accuracy: 45.88%\n",
            "Validation Accuracy after Epoch 25: 45.48%\n",
            "Epoch 26/45, Loss: 1.4767, Training Accuracy: 45.88%\n",
            "Validation Accuracy after Epoch 26: 45.40%\n",
            "Epoch 27/45, Loss: 1.4762, Training Accuracy: 45.89%\n",
            "Validation Accuracy after Epoch 27: 45.45%\n",
            "Epoch 28/45, Loss: 1.4758, Training Accuracy: 45.95%\n",
            "Validation Accuracy after Epoch 28: 45.50%\n",
            "Epoch 29/45, Loss: 1.4755, Training Accuracy: 45.93%\n",
            "Validation Accuracy after Epoch 29: 45.44%\n",
            "Epoch 30/45, Loss: 1.4753, Training Accuracy: 45.92%\n",
            "Validation Accuracy after Epoch 30: 45.48%\n",
            "Epoch 31/45, Loss: 1.4751, Training Accuracy: 45.91%\n",
            "Validation Accuracy after Epoch 31: 45.43%\n",
            "Epoch 32/45, Loss: 1.4748, Training Accuracy: 45.95%\n",
            "Validation Accuracy after Epoch 32: 45.54%\n",
            "Epoch 33/45, Loss: 1.4744, Training Accuracy: 45.96%\n",
            "Validation Accuracy after Epoch 33: 45.46%\n",
            "Epoch 34/45, Loss: 1.4742, Training Accuracy: 46.00%\n",
            "Validation Accuracy after Epoch 34: 45.50%\n",
            "Epoch 35/45, Loss: 1.4740, Training Accuracy: 45.98%\n",
            "Validation Accuracy after Epoch 35: 45.64%\n",
            "Epoch 36/45, Loss: 1.4738, Training Accuracy: 45.99%\n",
            "Validation Accuracy after Epoch 36: 45.56%\n",
            "Epoch 37/45, Loss: 1.4736, Training Accuracy: 45.99%\n",
            "Validation Accuracy after Epoch 37: 45.57%\n",
            "Epoch 38/45, Loss: 1.4734, Training Accuracy: 45.99%\n",
            "Validation Accuracy after Epoch 38: 45.55%\n",
            "Epoch 39/45, Loss: 1.4733, Training Accuracy: 45.96%\n",
            "Validation Accuracy after Epoch 39: 45.48%\n",
            "Epoch 40/45, Loss: 1.4730, Training Accuracy: 46.01%\n",
            "Validation Accuracy after Epoch 40: 45.61%\n",
            "Epoch 41/45, Loss: 1.4725, Training Accuracy: 46.02%\n",
            "Validation Accuracy after Epoch 41: 45.69%\n",
            "Epoch 42/45, Loss: 1.4724, Training Accuracy: 46.04%\n",
            "Validation Accuracy after Epoch 42: 45.69%\n",
            "Epoch 43/45, Loss: 1.4724, Training Accuracy: 46.04%\n",
            "Validation Accuracy after Epoch 43: 45.70%\n",
            "Epoch 44/45, Loss: 1.4724, Training Accuracy: 46.05%\n",
            "Validation Accuracy after Epoch 44: 45.70%\n",
            "Epoch 45/45, Loss: 1.4723, Training Accuracy: 46.03%\n",
            "Validation Accuracy after Epoch 45: 45.68%\n",
            "\n",
            "Training Updated FPGA-Optimized Model...\n",
            "Epoch 1/45, Loss: 1.7686, Training Accuracy: 38.57%\n",
            "Validation Accuracy after Epoch 1: 46.48%\n",
            "Epoch 2/45, Loss: 2.1138, Training Accuracy: 27.76%\n",
            "Validation Accuracy after Epoch 2: 36.74%\n",
            "Epoch 3/45, Loss: 1.8467, Training Accuracy: 33.46%\n",
            "Validation Accuracy after Epoch 3: 39.91%\n",
            "Epoch 4/45, Loss: 1.7359, Training Accuracy: 36.48%\n",
            "Validation Accuracy after Epoch 4: 42.11%\n",
            "Epoch 5/45, Loss: 1.6415, Training Accuracy: 39.46%\n",
            "Validation Accuracy after Epoch 5: 44.12%\n",
            "Epoch 6/45, Loss: 1.5774, Training Accuracy: 41.66%\n",
            "Validation Accuracy after Epoch 6: 43.76%\n",
            "Epoch 7/45, Loss: 1.5328, Training Accuracy: 43.20%\n",
            "Validation Accuracy after Epoch 7: 41.24%\n",
            "Epoch 8/45, Loss: 1.5088, Training Accuracy: 44.40%\n",
            "Validation Accuracy after Epoch 8: 47.07%\n",
            "Epoch 9/45, Loss: 1.4865, Training Accuracy: 45.21%\n",
            "Validation Accuracy after Epoch 9: 44.20%\n",
            "Epoch 10/45, Loss: 1.4483, Training Accuracy: 46.36%\n",
            "Validation Accuracy after Epoch 10: 28.58%\n",
            "Epoch 11/45, Loss: 1.4219, Training Accuracy: 47.31%\n",
            "Validation Accuracy after Epoch 11: 50.55%\n",
            "Epoch 12/45, Loss: 1.3980, Training Accuracy: 48.07%\n",
            "Validation Accuracy after Epoch 12: 50.97%\n",
            "Epoch 13/45, Loss: 1.3924, Training Accuracy: 48.40%\n",
            "Validation Accuracy after Epoch 13: 41.30%\n",
            "Epoch 14/45, Loss: 1.3816, Training Accuracy: 48.74%\n",
            "Validation Accuracy after Epoch 14: 51.45%\n",
            "Epoch 15/45, Loss: 1.3763, Training Accuracy: 48.90%\n",
            "Validation Accuracy after Epoch 15: 27.80%\n",
            "Epoch 16/45, Loss: 1.3555, Training Accuracy: 49.31%\n",
            "Validation Accuracy after Epoch 16: 51.71%\n",
            "Epoch 17/45, Loss: 1.3607, Training Accuracy: 49.45%\n",
            "Validation Accuracy after Epoch 17: 49.23%\n",
            "Epoch 18/45, Loss: 1.3558, Training Accuracy: 49.76%\n",
            "Validation Accuracy after Epoch 18: 51.91%\n",
            "Epoch 19/45, Loss: 1.3411, Training Accuracy: 49.82%\n",
            "Validation Accuracy after Epoch 19: 52.08%\n",
            "Epoch 20/45, Loss: 1.3282, Training Accuracy: 50.43%\n",
            "Validation Accuracy after Epoch 20: 52.38%\n",
            "Epoch 21/45, Loss: 1.2470, Training Accuracy: 53.80%\n",
            "Validation Accuracy after Epoch 21: 53.85%\n",
            "Epoch 22/45, Loss: 1.2444, Training Accuracy: 54.02%\n",
            "Validation Accuracy after Epoch 22: 54.03%\n",
            "Epoch 23/45, Loss: 1.2436, Training Accuracy: 54.01%\n",
            "Validation Accuracy after Epoch 23: 53.98%\n",
            "Epoch 24/45, Loss: 1.2431, Training Accuracy: 54.01%\n",
            "Validation Accuracy after Epoch 24: 53.97%\n",
            "Epoch 25/45, Loss: 1.2425, Training Accuracy: 54.08%\n",
            "Validation Accuracy after Epoch 25: 53.97%\n",
            "Epoch 26/45, Loss: 1.2421, Training Accuracy: 54.07%\n",
            "Validation Accuracy after Epoch 26: 54.04%\n",
            "Epoch 27/45, Loss: 1.2417, Training Accuracy: 54.08%\n",
            "Validation Accuracy after Epoch 27: 54.06%\n",
            "Epoch 28/45, Loss: 1.2414, Training Accuracy: 54.12%\n",
            "Validation Accuracy after Epoch 28: 54.05%\n",
            "Epoch 29/45, Loss: 1.2411, Training Accuracy: 54.12%\n",
            "Validation Accuracy after Epoch 29: 54.02%\n",
            "Epoch 30/45, Loss: 1.2409, Training Accuracy: 54.11%\n",
            "Validation Accuracy after Epoch 30: 54.02%\n",
            "Epoch 31/45, Loss: 1.2406, Training Accuracy: 54.16%\n",
            "Validation Accuracy after Epoch 31: 53.98%\n",
            "Epoch 32/45, Loss: 1.2403, Training Accuracy: 54.07%\n",
            "Validation Accuracy after Epoch 32: 54.05%\n",
            "Epoch 33/45, Loss: 1.2401, Training Accuracy: 54.16%\n",
            "Validation Accuracy after Epoch 33: 53.95%\n",
            "Epoch 34/45, Loss: 1.2399, Training Accuracy: 54.11%\n",
            "Validation Accuracy after Epoch 34: 54.01%\n",
            "Epoch 35/45, Loss: 1.2398, Training Accuracy: 54.15%\n",
            "Validation Accuracy after Epoch 35: 54.12%\n",
            "Epoch 36/45, Loss: 1.2394, Training Accuracy: 54.14%\n",
            "Validation Accuracy after Epoch 36: 54.11%\n",
            "Epoch 37/45, Loss: 1.2393, Training Accuracy: 54.18%\n",
            "Validation Accuracy after Epoch 37: 54.10%\n",
            "Epoch 38/45, Loss: 1.2391, Training Accuracy: 54.18%\n",
            "Validation Accuracy after Epoch 38: 54.07%\n",
            "Epoch 39/45, Loss: 1.2389, Training Accuracy: 54.18%\n",
            "Validation Accuracy after Epoch 39: 54.06%\n",
            "Epoch 40/45, Loss: 1.2386, Training Accuracy: 54.20%\n",
            "Validation Accuracy after Epoch 40: 54.06%\n",
            "Epoch 41/45, Loss: 1.2381, Training Accuracy: 54.22%\n",
            "Validation Accuracy after Epoch 41: 54.05%\n",
            "Epoch 42/45, Loss: 1.2381, Training Accuracy: 54.20%\n",
            "Validation Accuracy after Epoch 42: 54.05%\n",
            "Epoch 43/45, Loss: 1.2380, Training Accuracy: 54.19%\n",
            "Validation Accuracy after Epoch 43: 54.06%\n",
            "Epoch 44/45, Loss: 1.2380, Training Accuracy: 54.20%\n",
            "Validation Accuracy after Epoch 44: 54.09%\n",
            "Epoch 45/45, Loss: 1.2380, Training Accuracy: 54.20%\n",
            "Validation Accuracy after Epoch 45: 54.07%\n",
            "\n",
            "Comparison Results:\n",
            "Feature                        Updated Paper Model       Updated FPGA-Optimized Model\n",
            "================================================================================\n",
            "Convolution Type               Standard 3x3 & Asymmetric Depthwise Separable      \n",
            "Number of Layers               10 layers                 Fewer layers             \n",
            "Parameter Reduction Technique  Bottleneck & Asymmetric   Depthwise Separable      \n",
            "Skip Connections               Used                      Not used                 \n",
            "Pooling Strategy               Multiple Pooling Layers   Single Global Pooling    \n",
            "Fully Connected Layers         One Fully Connected       Minimized with Global Pooling\n",
            "Parameter Count                150296                    39893                    \n",
            "Inference Time (s/sample)      0.000281                  0.001129                 \n",
            "Accuracy                       45.6841                   54.0750                  \n",
            "Suitability for FPGA           Less suitable             Highly suitable          \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load RadioML dataset\n",
        "def load_radioml_data(filepath='RML2016.10a_dict.pkl'):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data_dict = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for key, value in data_dict.items():\n",
        "        mod_type, snr = key\n",
        "        data.append(value)\n",
        "        labels.extend([mod_type] * value.shape[0])\n",
        "\n",
        "    data = np.vstack(data)\n",
        "    label_set = sorted(list(set(labels)))\n",
        "    label_to_int = {label: i for i, label in enumerate(label_set)}\n",
        "    labels = np.array([label_to_int[label] for label in labels])\n",
        "\n",
        "    return data, labels, label_to_int\n",
        "\n",
        "# Dataset class for PyTorch\n",
        "class RadioMLDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        data = data[:, np.newaxis, :, :]  # Add channel dimension (1, 2, 1024)\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "# Depthwise Separable Convolution Layer\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# Original Paper Model\n",
        "class PaperAMCModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PaperAMCModel, self).__init__()\n",
        "\n",
        "        # Initial Convolution Layer with stride 2 to replace pooling\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=2, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Second Convolution Layer with stride 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=(1, 1), padding=0),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=(1, 1), padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=(1, 1), padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Global Pooling and Fully Connected Layer\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128, 24)  # Assuming 24 classes in the dataset\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Optimized Lightweight Model with Depthwise Separable Convolutions\n",
        "class OptimizedAMCModel(nn.Module):\n",
        "    def __init__(self, num_classes=24):\n",
        "        super(OptimizedAMCModel, self).__init__()\n",
        "\n",
        "        # Initial Depthwise Separable Convolution Layer\n",
        "        self.conv1 = DepthwiseSeparableConv(1, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Second Depthwise Separable Convolution Layer\n",
        "        self.conv2 = DepthwiseSeparableConv(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Global Pooling and Fully Connected Layer\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Weight Initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, scheduler, test_loader, epochs=45):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "        scheduler.step()\n",
        "        val_accuracy = evaluate_model(model, test_loader)\n",
        "        print(f\"Validation Accuracy after Epoch {epoch+1}: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return accuracy_score(all_labels, all_preds) * 100\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, test_loader):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in test_loader:\n",
        "            _ = model(inputs)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / len(test_loader.dataset)\n",
        "\n",
        "# Main function to compare models\n",
        "def main():\n",
        "    # Load data\n",
        "    data, labels, label_to_int = load_radioml_data('RML2016.10a_dict.pkl')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_dataset = RadioMLDataset(X_train, y_train)\n",
        "    test_dataset = RadioMLDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    models = {\n",
        "        \"Updated Paper Model\": PaperAMCModel(),\n",
        "        \"Updated FPGA-Optimized Model\": OptimizedAMCModel(num_classes=len(label_to_int))\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Training and Evaluation\n",
        "    for model_name, model in models.items():\n",
        "        model.apply(init_weights)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.01)\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        train_model(model, train_loader, criterion, optimizer, scheduler, test_loader, epochs=45)\n",
        "\n",
        "        # Evaluate accuracy\n",
        "        accuracy = evaluate_model(model, test_loader)\n",
        "\n",
        "        # Measure inference time\n",
        "        inference_time = measure_inference_time(model, test_loader)\n",
        "\n",
        "        # Count parameters\n",
        "        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        # Store results\n",
        "        results[model_name] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Inference Time (s/sample)\": inference_time,\n",
        "            \"Parameter Count\": param_count\n",
        "        }\n",
        "\n",
        "    # Display Comparison Table\n",
        "    print(\"\\nComparison Results:\")\n",
        "    print(f\"{'Feature':<30} {'Updated Paper Model':<25} {'Updated FPGA-Optimized Model':<25}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Convolution Type':<30} {'Standard 3x3 & Asymmetric':<25} {'Depthwise Separable':<25}\")\n",
        "    print(f\"{'Number of Layers':<30} {'10 layers':<25} {'Fewer layers':<25}\")\n",
        "    print(f\"{'Parameter Reduction Technique':<30} {'Bottleneck & Asymmetric':<25} {'Depthwise Separable':<25}\")\n",
        "    print(f\"{'Skip Connections':<30} {'Used':<25} {'Not used':<25}\")\n",
        "    print(f\"{'Pooling Strategy':<30} {'Multiple Pooling Layers':<25} {'Single Global Pooling':<25}\")\n",
        "    print(f\"{'Fully Connected Layers':<30} {'One Fully Connected':<25} {'Minimized with Global Pooling':<25}\")\n",
        "    print(f\"{'Parameter Count':<30} {results['Updated Paper Model']['Parameter Count']:<25} {results['Updated FPGA-Optimized Model']['Parameter Count']:<25}\")\n",
        "    print(f\"{'Inference Time (s/sample)':<30} {results['Updated Paper Model']['Inference Time (s/sample)']:<25.6f} {results['Updated FPGA-Optimized Model']['Inference Time (s/sample)']:<25.6f}\")\n",
        "    print(f\"{'Accuracy':<30} {results['Updated Paper Model']['Accuracy']:<25.4f} {results['Updated FPGA-Optimized Model']['Accuracy']:<25.4f}\")\n",
        "    print(f\"{'Suitability for FPGA':<30} {'Less suitable':<25} {'Highly suitable':<25}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cwyZr6ETSdT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}